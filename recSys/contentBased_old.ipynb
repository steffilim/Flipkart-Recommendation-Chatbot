{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump, load # to store matrix\n",
    "from surprise import accuracy\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import LeaveOneOut\n",
    "from surprise import NormalPredictor, SVD, AlgoBase, PredictionImpossible\n",
    "\n",
    "# from recommender_metrics import RecommenderMetrics\n",
    "# from evaluator import Evaluator\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.recommender_metrics import RecommenderMetrics\n",
    "from utility.evaluation_data import EvaluationData\n",
    "from utility.evaluated_algorithm import EvaluatedAlgorithm\n",
    "from utility.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_products shape (19906, 12)\n",
      "df_purchase shape (124968, 14)\n",
      "merged df shape (124956, 26)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124956 entries, 0 to 124955\n",
      "Data columns (total 26 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   Order ID                     124956 non-null  object \n",
      " 1   Product ID                   124956 non-null  object \n",
      " 2   Product Quantity             124956 non-null  int64  \n",
      " 3   Product Price Each           124956 non-null  float64\n",
      " 4   Order Total                  124956 non-null  float64\n",
      " 5   Order Date                   124956 non-null  object \n",
      " 6   Purchase Address             124956 non-null  object \n",
      " 7   User rating for the product  124956 non-null  float64\n",
      " 8   User ID                      124956 non-null  object \n",
      " 9   User Age                     124956 non-null  int64  \n",
      " 10  User Occupation              124956 non-null  object \n",
      " 11  User Income                  124956 non-null  int64  \n",
      " 12  User Interests               124956 non-null  object \n",
      " 13  User Ethnicity               124956 non-null  object \n",
      " 14  uniq_id                      124956 non-null  object \n",
      " 15  product_name                 124956 non-null  object \n",
      " 16  product_category_tree        124956 non-null  object \n",
      " 17  pid                          124956 non-null  object \n",
      " 18  retail_price                 124956 non-null  float64\n",
      " 19  discounted_price             124956 non-null  float64\n",
      " 20  discount                     124956 non-null  float64\n",
      " 21  description                  124956 non-null  object \n",
      " 22  overall_rating               124956 non-null  object \n",
      " 23  brand                        119143 non-null  object \n",
      " 24  product_specifications       124595 non-null  object \n",
      " 25  percentage_discount          124956 non-null  float64\n",
      "dtypes: float64(7), int64(3), object(16)\n",
      "memory usage: 24.8+ MB\n",
      "df.head None\n"
     ]
    }
   ],
   "source": [
    "# file paths for saving/loading \n",
    "lsa_matrix_file = 'lsa_matrix.joblib'\n",
    "product_data_file = '../newData/flipkart_cleaned.csv'\n",
    "purchase_history_file = '../newData/synthetic_v2.csv'\n",
    "\n",
    "### content based rec sys aims to recommend items based on similarity between items\n",
    "df_products = pd.read_csv(product_data_file)\n",
    "df_purchase = pd.read_csv(purchase_history_file)\n",
    "\n",
    "# merge df_purchase with df_products on Product ID\n",
    "df = pd.merge(df_purchase, df_products, left_on='Product ID', right_on='uniq_id')\n",
    "\n",
    "print(\"df_products shape\", df_products.shape)\n",
    "print(\"df_purchase shape\", df_purchase.shape)\n",
    "print(\"merged df shape\", df.shape)\n",
    "print(\"df.head\", df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_users 6597\n",
      "valid_items 11904\n"
     ]
    }
   ],
   "source": [
    "# keep users with >= ratings\n",
    "# Keep users with at least N ratings\n",
    "min_user_ratings = 10  # threshold\n",
    "user_counts = df['User ID'].value_counts()\n",
    "valid_users = user_counts[user_counts >= min_user_ratings].index\n",
    "df = df[df['User ID'].isin(valid_users)]\n",
    "print('valid_users',len(valid_users))\n",
    "\n",
    "# Keep items with at least N ratings\n",
    "min_item_ratings = 5  # example threshold\n",
    "item_counts = df['Product ID'].value_counts()\n",
    "valid_items = item_counts[item_counts >= min_item_ratings].index\n",
    "df = df[df['Product ID'].isin(valid_items)]\n",
    "print('valid_items',len(valid_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# randomly sample 10,000 rows from df\n",
    "sampled_df = df.sample(n=10000, random_state=42)  \n",
    "\n",
    "# check for users with no ratings\n",
    "user_counts = sampled_df['User ID'].value_counts()\n",
    "print(user_counts[user_counts == 0])\n",
    "\n",
    "# Check for items with no ratings\n",
    "item_counts = sampled_df['Product ID'].value_counts()\n",
    "print(item_counts[item_counts == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x375076d90>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Configuring Reader\n",
    "reader = Reader(rating_scale=(0,5))\n",
    "evaluation_data = Dataset.load_from_df(df[['User ID', 'Product ID', 'User rating for the product']], reader)\n",
    "\n",
    "evaluation_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popularity rankings calculations\n",
    "average_ratings = df.groupby('Product ID')['User rating for the product'].mean()\n",
    "\n",
    "df['average_ratings'] = df['Product ID'].map(average_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Product ID  average_ratings\n",
      "159    17087d2ddd19e19929c2ef485dd8c8e7         3.060000\n",
      "230    446876ebd53141d997ef790e6dd42d67         3.214286\n",
      "335    ed2b9ac3bd209d106366832fcea6f520         1.860000\n",
      "369    63f2dd3d90ff0c352b2a133c5ecaefcd         3.600000\n",
      "370    fb187233117b2eae554f47d2745fa954         2.440000\n",
      "...                                 ...              ...\n",
      "67546  31604a5b5c9b3399031b238a192996d6         2.560000\n",
      "68480  52de97eb4157cbe2dc1e9bd650b5348f         2.833333\n",
      "69670  1e36eb37f6c83aca2a2aab1ab0d90cd8         2.614286\n",
      "69981  0ae6a105ab4768e5da39dbe2399de9f1         1.540000\n",
      "77872  72fc51e3949799c0c694534325ad2cac         3.160000\n",
      "\n",
      "[389 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# evaluation_data = df_purchase\n",
    "# products_data = df_products\n",
    "popularity_rankings = df[['Product ID', 'average_ratings']]\n",
    "\n",
    "# Remove duplicate rows based on 'Product ID'\n",
    "popularity_rankings = popularity_rankings.drop_duplicates(subset='Product ID')\n",
    "\n",
    "print(popularity_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining product features to form content for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['product_name'].astype(str) + ' ' + df['product_category_tree'].astype(str) + ' ' + df['description'].astype(str) + ' ' + df['brand'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare evaluator\n",
    "Use evaluation data to test algorithms. Evaluator works with:\n",
    "- EvaluationData\n",
    "- EvaluatedAlgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of full trainset users: 1048\n",
      "Number of full trainset items: 389\n",
      "Number of trainset users: 898\n",
      "Number of trainset items: 389\n",
      "Size of testset: 513\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpopularity_rankings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/BT4103 - BZA Capstone Code/Flipkart-Recommendation-Chatbot/recSys/utility/evaluator.py:17\u001b[0m, in \u001b[0;36mEvaluator.__init__\u001b[0;34m(self, dataset, rankings)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset, rankings):\n\u001b[0;32m---> 17\u001b[0m     ed \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluationData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrankings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m ed\n",
      "File \u001b[0;32m~/Downloads/BT4103 - BZA Capstone Code/Flipkart-Recommendation-Chatbot/recSys/utility/evaluation_data.py:46\u001b[0m, in \u001b[0;36mEvaluationData.__init__\u001b[0;34m(self, data, popularity_rankings, verbose)\u001b[0m\n\u001b[1;32m     44\u001b[0m sim_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_based\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msims_algo \u001b[38;5;241m=\u001b[39m KNNBaseline(sim_options\u001b[38;5;241m=\u001b[39msim_options)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msims_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_train_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bza_capstone/lib/python3.11/site-packages/surprise/prediction_algorithms/knns.py:284\u001b[0m, in \u001b[0;36mKNNBaseline.fit\u001b[0;34m(self, trainset)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbu, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_baselines()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswitch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbu, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbi)\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_similarities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/bza_capstone/lib/python3.11/site-packages/surprise/prediction_algorithms/algo_base.py:248\u001b[0m, in \u001b[0;36mAlgoBase.compute_similarities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m similarity matrix...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 248\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43mconstruction_func\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone computing similarity matrix.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bza_capstone/lib/python3.11/site-packages/surprise/similarities.pyx:83\u001b[0m, in \u001b[0;36msurprise.similarities.cosine\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(evaluation_data, popularity_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Filler Algo to see if functions work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOwnAlgorithm(AlgoBase):\n",
    "    def __init__(self):\n",
    "\n",
    "        # Always call base method before doing anything.\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm MyOwnAlgorithm on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.4345  1.4803  1.4599  1.4603  1.4505  1.4571  0.0149  \n",
      "MAE (testset)     1.1927  1.2453  1.2100  1.2449  1.2305  1.2247  0.0205  \n",
      "Fit time          0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n",
      "Test time         0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.43445608, 1.48033336, 1.45994487, 1.46028731, 1.45047511]),\n",
       " 'test_mae': array([1.19270073, 1.24525547, 1.21      , 1.24487805, 1.2304878 ]),\n",
       " 'fit_time': (9.059906005859375e-06,\n",
       "  9.799003601074219e-05,\n",
       "  9.298324584960938e-05,\n",
       "  0.00011420249938964844,\n",
       "  9.489059448242188e-05),\n",
       " 'test_time': (0.0019421577453613281,\n",
       "  0.0003390312194824219,\n",
       "  0.0004930496215820312,\n",
       "  0.000621795654296875,\n",
       "  0.0003409385681152344)}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = MyOwnAlgorithm()\n",
    "cross_validate(algo, evaluation_data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedRecSysKNN(AlgoBase):\n",
    "\n",
    "    def __init__(self, k=40, movie_data=None, sim_options={}):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.k = k\n",
    "        self.movie_data = movie_data\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        \"\"\"\n",
    "        Fit function - calculate genre similarities\n",
    "        Args\n",
    "            trainset: the training set\n",
    "        Returns\n",
    "            self\n",
    "        \"\"\"\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        \n",
    "        self.compute_genre_similarities()\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def estimate(self, u, i):\n",
    "        \"\"\"\n",
    "        Estimate function: evaluate the ratings for an user & item\n",
    "        Process is:\n",
    "            - gather the ratings for the user u\n",
    "            - from the similarities (on items) between item i and the items\n",
    "            from the ratings gathered (neighboring items)\n",
    "            - order these neighbors \n",
    "            - calculate the average simmilarities score weighted by user ratings\n",
    "        Args\n",
    "            u: current user\n",
    "            i: current item\n",
    "        Returns\n",
    "            predicted ratings\n",
    "        \"\"\"\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unkown.')\n",
    "        \n",
    "        # Build up similarity scores between this item and everything the user rated\n",
    "        neighbors = []\n",
    "        for rating in self.trainset.ur[u]:\n",
    "            genre_similarity = self.genre_similarities[i,rating[0]]\n",
    "            neighbors.append( (genre_similarity, rating[1]) )\n",
    "        \n",
    "        # Extract the top-K most-similar ratings (k is set at init)\n",
    "        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[0])\n",
    "        \n",
    "        # Compute average sim score of K neighbors weighted by user ratings\n",
    "        sim_total = weighted_sum = 0\n",
    "        for (sim_score, rating) in k_neighbors:\n",
    "            if (sim_score > 0):\n",
    "                sim_total += sim_score\n",
    "                weighted_sum += sim_score * rating\n",
    "            \n",
    "        if (sim_total == 0):\n",
    "            raise PredictionImpossible('No neighbors')\n",
    "\n",
    "        predicted_rating = weighted_sum / sim_total\n",
    "\n",
    "        return predicted_rating\n",
    "    \n",
    "    def compute_genre_similarities(self):\n",
    "        # compute similarities\n",
    "        genre_columns = self.movie_data.columns[6:-1]\n",
    "        genres = self.movie_data[genre_columns]\n",
    "        # calculate cosine similarity using linear_kernel from sklearn\n",
    "        self.genre_similarities = linear_kernel(genres, genres)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m algo \u001b[38;5;241m=\u001b[39m ContentBasedRecSysKNN()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bza_capstone/lib/python3.11/site-packages/surprise/model_selection/validation.py:108\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(algo, data, measures, cv, return_train_measures, n_jobs, pre_dispatch, verbose)\u001b[0m\n\u001b[1;32m    102\u001b[0m cv \u001b[38;5;241m=\u001b[39m get_cv(cv)\n\u001b[1;32m    104\u001b[0m delayed_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    105\u001b[0m     delayed(fit_and_score)(algo, trainset, testset, measures, return_train_measures)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (trainset, testset) \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(data)\n\u001b[1;32m    107\u001b[0m )\n\u001b[0;32m--> 108\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m (test_measures_dicts, train_measures_dicts, fit_times, test_times) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mout)\n\u001b[1;32m    112\u001b[0m test_measures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/bza_capstone/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/bza_capstone/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/bza_capstone/lib/python3.11/site-packages/surprise/model_selection/validation.py:173\u001b[0m, in \u001b[0;36mfit_and_score\u001b[0;34m(algo, trainset, testset, measures, return_train_measures)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method that trains an algorithm and compute accuracy measures on\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03ma testset. Also report train and test times.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m        - The testing time in seconds.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m start_fit \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 173\u001b[0m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_fit\n\u001b[1;32m    175\u001b[0m start_test \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[64], line 18\u001b[0m, in \u001b[0;36mContentBasedRecSysKNN.fit\u001b[0;34m(self, trainset)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mFit function - calculate genre similarities\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mArgs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m AlgoBase\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m, trainset)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_genre_similarities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[64], line 65\u001b[0m, in \u001b[0;36mContentBasedRecSysKNN.compute_genre_similarities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_genre_similarities\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# compute similarities\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     genre_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmovie_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m[\u001b[38;5;241m6\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     66\u001b[0m     genres \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovie_data[genre_columns]\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# calculate cosine similarity using linear_kernel from sklearn\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "algo = ContentBasedRecSysKNN()\n",
    "cross_validate(algo, evaluation_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building User Profile\n",
    "Aggregate features of the products a user has interacted with, based on their purchase history and available ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining product features to form content for each product\n",
    "df['product_content'] = df['product_name'].astype(str) + ' ' + df['product_category_tree'].astype(str) + ' ' + df['description'].astype(str) + ' ' + df['brand'].astype(str)\n",
    "\n",
    "# group purchases by userID and aggregate product content and raings\n",
    "# user_data: purchases made by a specific user\n",
    "def aggregate_user_profile(user_data):\n",
    "    # weight product content by user ratings, normalize ratings\n",
    "    weighted_content = []\n",
    "    for index, row in user_data.iterrows():\n",
    "        content = row['product_content']\n",
    "        rating_weight = row['User rating for the product'] if pd.notna(row['User rating for the product']) else 0.1 # if rating not available, default to 0.1\n",
    "        weighted_content.append((content, rating_weight))\n",
    "\n",
    "    print(\"weighted content\", weighted_content)\n",
    "    \n",
    "    # aggregate product content: repeat each product's content according to rating weifht\n",
    "    all_content = ' '.join([content * round(weight) for content, weight in weighted_content])\n",
    "    print(\"all_content\", all_content)\n",
    "    return all_content\n",
    "\n",
    "# aggregate product contents for each user\n",
    "user_profiles = df.groupby('User ID').apply(aggregate_user_profile).reset_index()\n",
    "user_profiles.columns = ['User ID', 'user_profile_content']\n",
    "\n",
    "# vectorize the aggregated user profiles using CountVectorizer or TfidfTransformer\n",
    "vectorizer = CountVectorizer(max_df=0.85, stop_words='english')\n",
    "user_profiles_matrix = vectorizer.fit_transform(user_profiles['user_profile_content'])\n",
    "\n",
    "# apply TF-IDF transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "user_profiles_tfidf = tfidf_transformer.fit_transform(user_profiles_matrix)\n",
    "\n",
    "# At this point, `user_profiles_tfidf` is a matrix where each row is a user's profile,\n",
    "# represented as a vector of their aggregated product interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if LSA Matrix needs to be recalculated (if there is a modification to flipkart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsa_matrix does not exist, computing...\n",
      "Computing LSA matrix...\n",
      "LSA matrix saved to file.\n"
     ]
    }
   ],
   "source": [
    "recalculate_lsa = False\n",
    "\n",
    "if os.path.exists(lsa_matrix_file):\n",
    "    # Compare modification times\n",
    "    lsa_matrix_mtime = os.path.getmtime(lsa_matrix_file)\n",
    "    product_data_mtime = os.path.getmtime(product_data_file)\n",
    "\n",
    "    if product_data_mtime > lsa_matrix_mtime:\n",
    "        print(\"Product information database was updated, recalculating lsa_matrix...\")\n",
    "        recalculate_lsa = True\n",
    "else:\n",
    "    print(\"lsa_matrix does not exist, computing...\")\n",
    "    recalculate_lsa = True\n",
    "\n",
    "# Check if there is a need to compute LSA matrix\n",
    "if recalculate_lsa:\n",
    "    print(\"Computing LSA matrix...\")\n",
    "\n",
    "    # Create bag of words\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow = vectorizer.fit_transform(df['content'])\n",
    "\n",
    "    # Convert bag of words to TF-IDF\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf = tfidf_transformer.fit_transform(bow)\n",
    "\n",
    "    # Apply LSA \n",
    "    lsa = TruncatedSVD(n_components=100, algorithm='arpack')\n",
    "    lsa.fit(tfidf) # train lsa model\n",
    "    lsa_matrix = lsa.transform(tfidf) # project data onto learned components\n",
    "\n",
    "    # Save the computed LSA matrix to file\n",
    "    dump(lsa_matrix, lsa_matrix_file)\n",
    "    print(\"LSA matrix saved to file.\")\n",
    "else:\n",
    "    print(\"loading lsa_matrix from file...\")\n",
    "    lsa_matrix=load(lsa_matrix_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting user input to put into recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the user input\n",
    "user_product = input(\"Enter a product \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest match and score:  hello dolly women's leggings 90\n",
      "1772: hello dolly women's leggings\n",
      "1777: hello dolly women's leggings\n",
      "9343: hello dolly women's leggings\n",
      "9366: hello dolly women's leggings\n",
      "11367: hello dolly women's leggings\n",
      "11467: hello dolly women's leggings\n",
      "11604: hello dolly women's leggings\n",
      "14605: hello dolly women's leggings\n",
      "16236: hello dolly women's leggings\n",
      "Time taken to find recommendations: 3.31 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start timer after user input\n",
    "start_time = time.time()\n",
    "\n",
    "# Use fuzzy matching to find the closest product name\n",
    "match = process.extractOne(user_product, df['product_name'])\n",
    "closest_match = match[0]\n",
    "score = match[1]\n",
    "\n",
    "print(\"closest match and score: \", closest_match, score)\n",
    "\n",
    "if score < 70:\n",
    "    print(\"No close match found\")\n",
    "else:\n",
    "    # find the index of the closes product\n",
    "    product_index = df[df['product_name'] == closest_match].index[0]\n",
    "\n",
    "     # Compute the cosine similarities using the lsa_matrix\n",
    "    similarity_scores = cosine_similarity(lsa_matrix[product_index].reshape(1, -1), lsa_matrix)\n",
    "\n",
    "    # Get the top 10 most similar products\n",
    "    similar_products = list(enumerate(similarity_scores[0]))\n",
    "    sorted_similar_products = sorted(similar_products, key=lambda x: x[1], reverse=True)[1:10]\n",
    "\n",
    "    # Print the top 10 similar products\n",
    "    for i, score in sorted_similar_products:\n",
    "        print(\"{}: {}\".format(i, df.loc[i, 'product_name']))\n",
    "        # print(f\"Product: {df.loc[i, 'product_name']} | Price: {df.loc[i, 'retail_price']} | Rating: {df.loc[i, 'overall_rating']} | Similarity: {score}\")\n",
    "\n",
    "\n",
    "# End timer for the entire program\n",
    "end_time = time.time()\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken to find recommendations: {:.2f} seconds\".format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bza_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
